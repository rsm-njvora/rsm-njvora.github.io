{
  "hash": "94144a2fa8a368720209ef6e261df0f7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Replication of Karlan and List (2007)\"\nauthor: \"Nivan Vora\"\ndate: April 23 2025\ncallout-appearance: minimal \n---\n\n\n\n## Introduction\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2). \n\n_to do: Participants were split into a **control group** and a **treatment group**:\n\n- **Control group**: Received a standard letter with no mention of a match.\n- **Treatment group**: Received a letter stating that a \"concerned fellow member\" would match their donation. These individuals were randomly assigned to one of three **match ratios**:\n  - **1:1** — Every $1 donated is matched with $1.\n  - **2:1** — Every $1 donated is matched with $2.\n  - **3:1** — Every $1 donated is matched with $3.\n\nAdditional randomization dimensions included:\n- **Maximum matching grant size**: $25,000, $50,000, $100,000, or unspecified.\n- **Suggested donation amount**: Equal to, 1.25×, or 1.5× of the individual’s previous highest donation.\n\nKey findings from the study:\n- Mentioning a matching grant **increased donation response rate by 22%** and **revenue per solicitation by 19%**.\n- Surprisingly, **higher match ratios (2:1, 3:1) did not yield significantly greater giving** than the 1:1 match.\n- **Heterogeneous treatment effects** were found—donors in “red states” (those that voted for George W. Bush in 2004) responded significantly more to the matching grant than those in “blue states”.\n\nThis experiment provides robust evidence for the effectiveness of matching grants in charitable giving and raises important questions about donor psychology, price sensitivity, and the contextual framing of donation requests.\n\nThis project seeks to replicate their results.\n--\n\n## Data\n\n### Description\n\n::: {#6d623e9a .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_describe = df.describe()\ndf.describe(include='all')\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>treatment</th>\n      <th>control</th>\n      <th>ratio</th>\n      <th>ratio2</th>\n      <th>ratio3</th>\n      <th>size</th>\n      <th>size25</th>\n      <th>size50</th>\n      <th>size100</th>\n      <th>sizeno</th>\n      <th>...</th>\n      <th>redcty</th>\n      <th>bluecty</th>\n      <th>pwhite</th>\n      <th>pblack</th>\n      <th>page18_39</th>\n      <th>ave_hh_sz</th>\n      <th>median_hhincome</th>\n      <th>powner</th>\n      <th>psch_atlstba</th>\n      <th>pop_propurban</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>50083.000000</td>\n      <td>50083.000000</td>\n      <td>50083</td>\n      <td>50083.000000</td>\n      <td>50083.000000</td>\n      <td>50083</td>\n      <td>50083.000000</td>\n      <td>50083.000000</td>\n      <td>50083.000000</td>\n      <td>50083.000000</td>\n      <td>...</td>\n      <td>49978.000000</td>\n      <td>49978.000000</td>\n      <td>48217.000000</td>\n      <td>48047.000000</td>\n      <td>48217.000000</td>\n      <td>48221.000000</td>\n      <td>48209.000000</td>\n      <td>48214.000000</td>\n      <td>48215.000000</td>\n      <td>48217.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Control</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Control</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16687</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16687</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.666813</td>\n      <td>0.333187</td>\n      <td>NaN</td>\n      <td>0.222311</td>\n      <td>0.222211</td>\n      <td>NaN</td>\n      <td>0.166723</td>\n      <td>0.166623</td>\n      <td>0.166723</td>\n      <td>0.166743</td>\n      <td>...</td>\n      <td>0.510245</td>\n      <td>0.488715</td>\n      <td>0.819599</td>\n      <td>0.086710</td>\n      <td>0.321694</td>\n      <td>2.429012</td>\n      <td>54815.700533</td>\n      <td>0.669418</td>\n      <td>0.391661</td>\n      <td>0.871968</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.471357</td>\n      <td>0.471357</td>\n      <td>NaN</td>\n      <td>0.415803</td>\n      <td>0.415736</td>\n      <td>NaN</td>\n      <td>0.372732</td>\n      <td>0.372643</td>\n      <td>0.372732</td>\n      <td>0.372750</td>\n      <td>...</td>\n      <td>0.499900</td>\n      <td>0.499878</td>\n      <td>0.168560</td>\n      <td>0.135868</td>\n      <td>0.103039</td>\n      <td>0.378105</td>\n      <td>22027.316665</td>\n      <td>0.193405</td>\n      <td>0.186599</td>\n      <td>0.258633</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.009418</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5000.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.755845</td>\n      <td>0.014729</td>\n      <td>0.258311</td>\n      <td>2.210000</td>\n      <td>39181.000000</td>\n      <td>0.560222</td>\n      <td>0.235647</td>\n      <td>0.884929</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.872797</td>\n      <td>0.036554</td>\n      <td>0.305534</td>\n      <td>2.440000</td>\n      <td>50673.000000</td>\n      <td>0.712296</td>\n      <td>0.373744</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.938827</td>\n      <td>0.090882</td>\n      <td>0.369132</td>\n      <td>2.660000</td>\n      <td>66005.000000</td>\n      <td>0.816798</td>\n      <td>0.530036</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.989622</td>\n      <td>0.997544</td>\n      <td>5.270000</td>\n      <td>200001.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows × 51 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n\n### Balance Test \n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n::: {#77ad2497 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\n\ndf,_  = pyreadstat.read_dta(\"karlan_list_2007.dta\")\n\nvariables_to_test = ['mrm2', 'years', 'ave_hh_sz', 'female', 'couple']\n\nfor var in variables_to_test:\n    df[var] = pd.to_numeric(df[var], errors='coerce')\n\nresults = []\n\nfor var in variables_to_test:\n    treat = df[df['treatment'] == 1][var].dropna()\n    control = df[df['treatment'] == 0][var].dropna()\n\n    t_stat, p_val_ttest = ttest_ind(treat, control, nan_policy='omit')\n\n    reg_df = df[['treatment', var]].dropna()\n    X = sm.add_constant(reg_df['treatment'])\n    y = reg_df[var]\n    model = sm.OLS(y, X).fit()\n    coef = model.params['treatment']\n    t_val_reg = model.tvalues['treatment']\n    p_val_reg = model.pvalues['treatment']\n\n    results.append({\n        'variable': var,\n        'mean_treat': round(treat.mean(), 3),\n        'mean_control': round(control.mean(), 3),\n        't_stat_ttest': round(t_stat, 3),\n        'p_value_ttest': round(p_val_ttest, 3),\n        'regression_coef': round(coef, 3),\n        't_stat_regression': round(t_val_reg, 3),\n        'p_value_regression': round(p_val_reg, 3),\n        'significant_95pct': p_val_ttest < 0.05 and p_val_reg < 0.05\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    variable  mean_treat  mean_control  t_stat_ttest  p_value_ttest  \\\n0       mrm2      13.012        12.998         0.119          0.905   \n1      years       6.078         6.136        -1.103          0.270   \n2  ave_hh_sz       2.430         2.427         0.824          0.410   \n3     female       0.275         0.283        -1.758          0.079   \n4     couple       0.091         0.093        -0.584          0.559   \n\n   regression_coef  t_stat_regression  p_value_regression  significant_95pct  \n0            0.014              0.119               0.905              False  \n1           -0.058             -1.103               0.270              False  \n2            0.003              0.824               0.410              False  \n3           -0.008             -1.758               0.079              False  \n4           -0.002             -0.584               0.559              False  \n```\n:::\n:::\n\n\nTo evaluate the validity of the randomization, we tested several non-outcome variables — such as months since last donation, years since initial donation, gender, and household characteristics — to see whether there were statistically significant differences between the treatment and control groups.\n\nFor each variable, we conducted:\n- A **two-sample t-test**, and\n- A **linear regression** of the form `variable ~ treatment`.\n\nAcross all tested variables, **none showed statistically significant differences** between the treatment and control groups at the 95% confidence level (p-values > 0.05). This confirms that the treatment assignment was successfully randomized.\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\n::: {#c9b829ae .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean()\ndonation_labels = ['Control', 'Treatment']\n\nplt.figure(figsize=(6, 4))\nplt.bar(donation_labels, donation_rates, color=['gray', 'skyblue'])\nplt.title(\"Proportion of People Who Donated\")\nplt.ylabel(\"Donation Rate\")\nplt.ylim(0, 0.03)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw1_questions_files/figure-html/cell-4-output-1.png){width=565 height=373}\n:::\n:::\n\n\n::: {#fc456967 .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ntreat = df[df['treatment'] == 1]['gave'].dropna()\ncontrol = df[df['treatment'] == 0]['gave'].dropna()\nt_stat, p_val = ttest_ind(treat, control, nan_policy='omit')\n\nreg_df = df[['treatment', 'gave']].dropna()\nX = sm.add_constant(reg_df['treatment'])\ny = reg_df['gave']\nmodel = sm.OLS(y, X).fit()\n\nt_test_result = {\n    \"t_statistic\": round(t_stat, 4),\n    \"p_value\": round(p_val, 4),\n    \"mean_treatment\": round(treat.mean(), 4),\n    \"mean_control\": round(control.mean(), 4)\n}\n\nregression_summary = model.summary()\n\nt_test_result, regression_summary\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n({'t_statistic': 3.1014,\n  'p_value': 0.0019,\n  'mean_treatment': 0.022,\n  'mean_control': 0.0179},\n <class 'statsmodels.iolib.summary.Summary'>\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Wed, 23 Apr 2025   Prob (F-statistic):            0.00193\n Time:                        20:32:02   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n const          0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n```\n:::\n:::\n\n\n::: {#ec6cc30a .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = df[['treatment', 'gave']].dropna()\n\nprobit_model = sm.Probit(df_clean['gave'], sm.add_constant(df_clean['treatment'])).fit(disp=False)\n\nsummary_stats = {\n    \"Pseudo R-squared\": probit_model.prsquared,\n    \"Log-Likelihood\": probit_model.llf,\n    \"AIC\": probit_model.aic,\n    \"BIC\": probit_model.bic,\n    \"Observations\": int(probit_model.nobs),\n    \"Coef (treatment)\": probit_model.params['treatment'],\n    \"Std Err (treatment)\": probit_model.bse['treatment'],\n    \"z-stat (treatment)\": probit_model.tvalues['treatment'],\n    \"P>|z| (treatment)\": probit_model.pvalues['treatment'],\n    \"95% CI (treatment)\": probit_model.conf_int().loc['treatment'].tolist()\n}\n\nmarginal_effect = probit_model.get_margeff(at='overall').summary_frame().loc['treatment', 'dy/dx']\n\nsummary_stats[\"Marginal Effect (dy/dx)\"] = marginal_effect\n\nsummary_stats\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n{'Pseudo R-squared': 0.0009782722838191926,\n 'Log-Likelihood': -5030.501164143209,\n 'AIC': 10065.002328286419,\n 'BIC': 10082.645202102685,\n 'Observations': 50083,\n 'Coef (treatment)': 0.08678462244745852,\n 'Std Err (treatment)': 0.027878757437573513,\n 'z-stat (treatment)': 3.1129300737949963,\n 'P>|z| (treatment)': 0.001852399014778513,\n '95% CI (treatment)': [0.03214326193608627, 0.14142598295883077],\n 'Marginal Effect (dy/dx)': 0.004313211579633209}\n```\n:::\n:::\n\n\nThis Probit model confirms that individuals who received a matched donation offer were significantly more likely to donate. The marginal effect suggests a 0.43 percentage point increase in the probability of donating—small but statistically significant (p < 0.01).\n\n🧪 Statistical Approach & Interpretation\n\nIn this analysis, we test whether being assigned to the treatment group (which received a matched donation offer) significantly affects the likelihood of making a charitable donation.\n\nWe use two methods:\n- **T-test**: Compares the mean donation rates between treatment and control groups.\n- **Bivariate Linear Regression**: Regresses the binary outcome `gave` on the `treatment` variable to estimate the average treatment effect.\n\n📊 Key Results\n- The mean donation rate in the **treatment group** is higher than in the **control group**.\n- Both the t-test and regression confirm this difference is **statistically significant at the 5% level**.\n- The effect size is modest (a few tenths of a percentage point), but **positive and consistent** across methods.\n\n📌 Interpretation\nThese results suggest that **offering a matching donation increases the likelihood of giving**. Even though the increase is small, it is statistically meaningful and aligns with behavioral insights: people respond to perceived value enhancement when their donation is matched.\n\nThis supports the idea that **subtle psychological nudges—like matched giving—can effectively influence charitable behavior**. The very low R-squared value is typical for binary outcome models with limited predictors and does not reduce the credibility of the treatment effect.\n\n\n\n### Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n::: {#9b4c0658 .cell execution_count=6}\n``` {.python .cell-code}\nimport pandas as pd\nfrom scipy.stats import ttest_ind\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf['ratio_str'] = df['ratio'].replace({1: '1:1', 2: '2:1', 3: '3:1'})\n\nmatch_df = df[df['treatment'] == 1][['gave', 'ratio_str']].dropna()\n\ngave_1to1 = match_df[match_df['ratio_str'] == '1:1']['gave']\ngave_2to1 = match_df[match_df['ratio_str'] == '2:1']['gave']\ngave_3to1 = match_df[match_df['ratio_str'] == '3:1']['gave']\n\ntests = {\n    \"2:1 vs 1:1\": ttest_ind(gave_2to1, gave_1to1, nan_policy='omit'),\n    \"3:1 vs 1:1\": ttest_ind(gave_3to1, gave_1to1, nan_policy='omit'),\n    \"3:1 vs 2:1\": ttest_ind(gave_3to1, gave_2to1, nan_policy='omit')\n}\n\nprint(\"=== T-Test Results on Match Ratios ===\")\nfor label, result in tests.items():\n    t_stat, p_val = result\n    significance = \"✅ Significant\" if p_val < 0.05 else \"❌ Not Significant\"\n    print(f\"{label:<15} | t = {t_stat:.4f}, p = {p_val:.4f} | {significance}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n=== T-Test Results on Match Ratios ===\n2:1 vs 1:1      | t = 0.9650, p = 0.3345 | ❌ Not Significant\n3:1 vs 1:1      | t = 1.0150, p = 0.3101 | ❌ Not Significant\n3:1 vs 2:1      | t = 0.0501, p = 0.9600 | ❌ Not Significant\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_39959/955975056.py:5: FutureWarning:\n\nThe behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n\n```\n:::\n:::\n\n\n::: {#73018788 .cell execution_count=7}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_ratio = df[df['treatment'] == 1].copy()\n\ndf_ratio['ratio1'] = (df_ratio['ratio'] == 1).astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\nX = df_ratio[['ratio2', 'ratio3']]  \nX = sm.add_constant(X)\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\nregression_summary = model.summary()\nregression_summary\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>gave</td>       <th>  R-squared:         </th>  <td>   0.000</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>  -0.000</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>  0.6454</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Wed, 23 Apr 2025</td> <th>  Prob (F-statistic):</th>   <td> 0.524</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>20:32:03</td>     <th>  Log-Likelihood:    </th>  <td>  16688.</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td> 33396</td>      <th>  AIC:               </th> <td>-3.337e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td> 33393</td>      <th>  BIC:               </th> <td>-3.334e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th>  <td>    0.0207</td> <td>    0.001</td> <td>   14.912</td> <td> 0.000</td> <td>    0.018</td> <td>    0.023</td>\n</tr>\n<tr>\n  <th>ratio2</th> <td>    0.0019</td> <td>    0.002</td> <td>    0.958</td> <td> 0.338</td> <td>   -0.002</td> <td>    0.006</td>\n</tr>\n<tr>\n  <th>ratio3</th> <td>    0.0020</td> <td>    0.002</td> <td>    1.008</td> <td> 0.313</td> <td>   -0.002</td> <td>    0.006</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>38963.957</td> <th>  Durbin-Watson:     </th>  <td>   1.995</td>  \n</tr>\n<tr>\n  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>2506478.937</td>\n</tr>\n<tr>\n  <th>Skew:</th>           <td> 6.511</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n</tr>\n<tr>\n  <th>Kurtosis:</th>       <td>43.394</td>   <th>  Cond. No.          </th>  <td>    3.73</td>  \n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nThe regression results show that neither the 2:1 nor the 3:1 match ratios led to a statistically significant increase in donation likelihood compared to the 1:1 match. While both ratio2 and ratio3 have positive coefficients, their p-values (0.338 and 0.313) are well above conventional significance thresholds. This suggests that offering a higher match doesn’t meaningfully change the probability that someone donates, at least in this context.\n\n::: {#de9553f6 .cell execution_count=8}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf['ratio_str'] = df['ratio'].replace({1: '1:1', 2: '2:1', 3: '3:1'})\n\nmatch_df = df[df['treatment'] == 1][['gave', 'ratio_str']].dropna()\n\ngave_1to1 = match_df[match_df['ratio_str'] == '1:1']['gave']\ngave_2to1 = match_df[match_df['ratio_str'] == '2:1']['gave']\ngave_3to1 = match_df[match_df['ratio_str'] == '3:1']['gave']\n\nrate_1to1 = gave_1to1.mean()\nrate_2to1 = gave_2to1.mean()\nrate_3to1 = gave_3to1.mean()\n\ndiff_2_vs_1_data = rate_2to1 - rate_1to1\ndiff_3_vs_2_data = rate_3to1 - rate_2to1\n\ndf['ratio2'] = (df['ratio'] == 2).astype(int)\ndf['ratio3'] = (df['ratio'] == 3).astype(int)\nreg_df = df[df['treatment'] == 1][['gave', 'ratio2', 'ratio3']].dropna()\n\nimport statsmodels.api as sm\nX = sm.add_constant(reg_df[['ratio2', 'ratio3']])\ny = reg_df['gave']\nmodel = sm.OLS(y, X).fit()\n\ndiff_2_vs_1_reg = model.params['ratio2']\ndiff_3_vs_2_reg = model.params['ratio3'] - model.params['ratio2']\n\n{\n    \"Response Rate Difference (2:1 - 1:1) [Data]\": round(diff_2_vs_1_data, 4),\n    \"Response Rate Difference (3:1 - 2:1) [Data]\": round(diff_3_vs_2_data, 4),\n    \"Coefficient Difference (2:1 - 1:1) [Regression]\": round(diff_2_vs_1_reg, 4),\n    \"Coefficient Difference (3:1 - 2:1) [Regression]\": round(diff_3_vs_2_reg, 4)\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_39959/1367793913.py:5: FutureWarning:\n\nThe behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n{'Response Rate Difference (2:1 - 1:1) [Data]': 0.0019,\n 'Response Rate Difference (3:1 - 2:1) [Data]': 0.0001,\n 'Coefficient Difference (2:1 - 1:1) [Regression]': 0.0019,\n 'Coefficient Difference (3:1 - 2:1) [Regression]': 0.0001}\n```\n:::\n:::\n\n\nBased on both the direct data comparisons and the regression coefficients, the differences in response rates between match sizes are extremely small. The move from a 1:1 to a 2:1 match increases the donation rate by just 0.0019 (0.19 percentage points), and the jump from 2:1 to 3:1 adds only 0.0001 (0.01 percentage points). These effects are negligible in practice and statistically insignificant. The conclusion is clear: increasing the match ratio beyond 1:1 does not meaningfully improve donation rates. The mere presence of a match appears to matter, but making the match more generous does not lead to further increases in giving\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n\n_todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_\n\n::: {#da24dc1d .cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport pyreadstat\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_amount = df[['gave', 'amount', 'treatment']].dropna()\n\namount_treat = df_amount[df_amount['treatment'] == 1]['amount']\namount_control = df_amount[df_amount['treatment'] == 0]['amount']\nt_stat, p_value = ttest_ind(amount_treat, amount_control, nan_policy='omit')\n\nX = sm.add_constant(df_amount['treatment'])\ny = df_amount['amount']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_positive_donors = df_amount[df_amount['gave'] == 1].copy()\n\namount_treat = df_positive_donors[df_positive_donors['treatment'] == 1]['amount']\namount_control = df_positive_donors[df_positive_donors['treatment'] == 0]['amount']\nt_stat, p_value = ttest_ind(amount_treat, amount_control, nan_policy='omit')\n\nX = sm.add_constant(df_positive_donors['treatment'])\ny = df_positive_donors['amount']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\ntreatment_donors = df_positive_donors[df_positive_donors['treatment'] == 1]['amount']\ncontrol_donors = df_positive_donors[df_positive_donors['treatment'] == 0]['amount']\n\nmean_treatment = treatment_donors.mean()\nmean_control = control_donors.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\naxes[0].hist(treatment_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = ${mean_treatment:.2f}\")\naxes[0].set_title(\"Treatment Group (Donors Only)\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\naxes[1].hist(control_donors, bins=30, color='lightgray', edgecolor='black')\naxes[1].axvline(mean_control, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = ${mean_control:.2f}\")\naxes[1].set_title(\"Control Group (Donors Only)\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nprint(plt.show())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):             0.0628\nTime:                        20:32:03   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 23 Apr 2025   Prob (F-statistic):              0.561\nTime:                        20:32:04   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw1_questions_files/figure-html/cell-10-output-2.png){width=1141 height=468}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNone\n```\n:::\n:::\n\n\nThe analysis shows that while the treatment group gave slightly more on average than the control group when everyone is included, the difference isn't statistically significant at conventional levels. When we look only at those who actually donated, the control group gave more on average than the treatment group, and again, the difference isn’t significant. The histograms reinforce this, showing very similar distributions for both groups. This suggests that the match offer might help get more people to donate, but it doesn't appear to influence how much they give once they've made the decision to contribute.\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\n_to do:  Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means._\n\n::: {#af314263 .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ncontrol_draws = np.random.binomial(1, 0.018, size=10000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\ndiffs = treatment_draws - control_draws\n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(0.004, color='red', linestyle='--', label='True Mean Difference (0.022 - 0.018)')\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Law of Large Numbers: Convergence of Mean Difference\")\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw1_questions_files/figure-html/cell-11-output-1.png){width=821 height=449}\n:::\n:::\n\n\nThis plot provides visual evidence of the **Law of Large Numbers** in action. Initially, the **cumulative average of differences** fluctuates significantly due to high variability in small sample comparisons. However, as the number of simulations increases, the cumulative average begins to **stabilize** and converge toward the **true mean difference** of 0.004 (i.e., 0.022 − 0.018), indicated by the red dashed line.\n\nBy the time we reach several thousand simulations, the cumulative average consistently hovers around the true difference, confirming that with enough observations, the sample mean reliably estimates the population mean difference. This reinforces the importance of **large sample sizes** when estimating small treatment effects.\n\n\n\n### Central Limit Theorem\n\n::: {#98c52117 .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nsample_sizes = [50, 200, 500, 1000]\ntrue_diff = 0.022 - 0.018\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, 0.018, size=n)\n        treatment_sample = np.random.binomial(1, 0.022, size=n)\n        mean_diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(mean_diff)\n    \n    axes[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='black', linestyle='--')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label=f'True Diff = {true_diff:.4f}')\n    axes[i].set_title(f'Sample Size: {n}')\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Sampling Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw1_questions_files/figure-html/cell-12-output-1.png){width=1331 height=945}\n:::\n:::\n\n\nIn the histogram for **sample size = 50**, zero is approximately in the **middle** of the distribution, which means that with small samples, it’s difficult to distinguish whether the treatment has a meaningful effect—the distribution is too wide, and zero is a plausible average difference just by chance.\n\nAs the **sample size increases** to 200, 500, and especially 1000, zero moves toward the **tails** of the distribution. This suggests that with more data, the sample differences become more precise and center more tightly around the **true mean difference** (0.004), making it less likely that the observed effect is due to chance. At large sample sizes, **zero is no longer a likely value**, reinforcing confidence that the treatment has a small but real effect.\n\n",
    "supporting": [
      "hw1_questions_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}