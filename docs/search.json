[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nivan Vora",
    "section": "",
    "text": "Welcome to my Website!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\n_to do: Participants were split into a control group and a treatment group:\n\nControl group: Received a standard letter with no mention of a match.\nTreatment group: Received a letter stating that a “concerned fellow member” would match their donation. These individuals were randomly assigned to one of three match ratios:\n\n1:1 — Every $1 donated is matched with $1.\n2:1 — Every $1 donated is matched with $2.\n3:1 — Every $1 donated is matched with $3.\n\n\nAdditional randomization dimensions included: - Maximum matching grant size: $25,000, $50,000, $100,000, or unspecified. - Suggested donation amount: Equal to, 1.25×, or 1.5× of the individual’s previous highest donation.\nKey findings from the study: - Mentioning a matching grant increased donation response rate by 22% and revenue per solicitation by 19%. - Surprisingly, higher match ratios (2:1, 3:1) did not yield significantly greater giving than the 1:1 match. - Heterogeneous treatment effects were found—donors in “red states” (those that voted for George W. Bush in 2004) responded significantly more to the matching grant than those in “blue states”.\nThis experiment provides robust evidence for the effectiveness of matching grants in charitable giving and raises important questions about donor psychology, price sensitivity, and the contextual framing of donation requests."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\n_to do: Participants were split into a control group and a treatment group:\n\nControl group: Received a standard letter with no mention of a match.\nTreatment group: Received a letter stating that a “concerned fellow member” would match their donation. These individuals were randomly assigned to one of three match ratios:\n\n1:1 — Every $1 donated is matched with $1.\n2:1 — Every $1 donated is matched with $2.\n3:1 — Every $1 donated is matched with $3.\n\n\nAdditional randomization dimensions included: - Maximum matching grant size: $25,000, $50,000, $100,000, or unspecified. - Suggested donation amount: Equal to, 1.25×, or 1.5× of the individual’s previous highest donation.\nKey findings from the study: - Mentioning a matching grant increased donation response rate by 22% and revenue per solicitation by 19%. - Surprisingly, higher match ratios (2:1, 3:1) did not yield significantly greater giving than the 1:1 match. - Heterogeneous treatment effects were found—donors in “red states” (those that voted for George W. Bush in 2004) responded significantly more to the matching grant than those in “blue states”.\nThis experiment provides robust evidence for the effectiveness of matching grants in charitable giving and raises important questions about donor psychology, price sensitivity, and the contextual framing of donation requests."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#this-project-seeks-to-replicate-their-results.",
    "href": "blog/project1/hw1_questions.html#this-project-seeks-to-replicate-their-results.",
    "title": "A Replication of Karlan and List (2007)",
    "section": "This project seeks to replicate their results.",
    "text": "This project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport numpy as np\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_describe = df.describe()\ndf.describe(include='all')\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nunique\nNaN\nNaN\n4\nNaN\nNaN\n5\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\nNaN\nControl\nNaN\nNaN\nControl\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\nNaN\n16687\nNaN\nNaN\n16687\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n0.666813\n0.333187\nNaN\n0.222311\n0.222211\nNaN\n0.166723\n0.166623\n0.166723\n0.166743\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\nNaN\n0.415803\n0.415736\nNaN\n0.372732\n0.372643\n0.372732\n0.372750\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n11 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\n\ndf,_  = pyreadstat.read_dta(\"karlan_list_2007.dta\")\n\nvariables_to_test = ['mrm2', 'years', 'ave_hh_sz', 'female', 'couple']\n\nfor var in variables_to_test:\n    df[var] = pd.to_numeric(df[var], errors='coerce')\n\nresults = []\n\nfor var in variables_to_test:\n    treat = df[df['treatment'] == 1][var].dropna()\n    control = df[df['treatment'] == 0][var].dropna()\n\n    t_stat, p_val_ttest = ttest_ind(treat, control, nan_policy='omit')\n\n    reg_df = df[['treatment', var]].dropna()\n    X = sm.add_constant(reg_df['treatment'])\n    y = reg_df[var]\n    model = sm.OLS(y, X).fit()\n    coef = model.params['treatment']\n    t_val_reg = model.tvalues['treatment']\n    p_val_reg = model.pvalues['treatment']\n\n    results.append({\n        'variable': var,\n        'mean_treat': round(treat.mean(), 3),\n        'mean_control': round(control.mean(), 3),\n        't_stat_ttest': round(t_stat, 3),\n        'p_value_ttest': round(p_val_ttest, 3),\n        'regression_coef': round(coef, 3),\n        't_stat_regression': round(t_val_reg, 3),\n        'p_value_regression': round(p_val_reg, 3),\n        'significant_95pct': p_val_ttest &lt; 0.05 and p_val_reg &lt; 0.05\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df)\n\n    variable  mean_treat  mean_control  t_stat_ttest  p_value_ttest  \\\n0       mrm2      13.012        12.998         0.119          0.905   \n1      years       6.078         6.136        -1.103          0.270   \n2  ave_hh_sz       2.430         2.427         0.824          0.410   \n3     female       0.275         0.283        -1.758          0.079   \n4     couple       0.091         0.093        -0.584          0.559   \n\n   regression_coef  t_stat_regression  p_value_regression  significant_95pct  \n0            0.014              0.119               0.905              False  \n1           -0.058             -1.103               0.270              False  \n2            0.003              0.824               0.410              False  \n3           -0.008             -1.758               0.079              False  \n4           -0.002             -0.584               0.559              False  \n\n\nTo evaluate the validity of the randomization, we tested several non-outcome variables — such as months since last donation, years since initial donation, gender, and household characteristics — to see whether there were statistically significant differences between the treatment and control groups.\nFor each variable, we conducted: - A two-sample t-test, and - A linear regression of the form variable ~ treatment.\nAcross all tested variables, none showed statistically significant differences between the treatment and control groups at the 95% confidence level (p-values &gt; 0.05). This confirms that the treatment assignment was successfully randomized."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean()\ndonation_labels = ['Control', 'Treatment']\n\nplt.figure(figsize=(6, 4))\nplt.bar(donation_labels, donation_rates, color=['gray', 'skyblue'])\nplt.title(\"Proportion of People Who Donated\")\nplt.ylabel(\"Donation Rate\")\nplt.ylim(0, 0.03)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat \n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ntreat = df[df['treatment'] == 1]['gave'].dropna()\ncontrol = df[df['treatment'] == 0]['gave'].dropna()\nt_stat, p_val = ttest_ind(treat, control, nan_policy='omit')\n\nreg_df = df[['treatment', 'gave']].dropna()\nX = sm.add_constant(reg_df['treatment'])\ny = reg_df['gave']\nmodel = sm.OLS(y, X).fit()\n\nt_test_result = {\n    \"t_statistic\": round(t_stat, 4),\n    \"p_value\": round(p_val, 4),\n    \"mean_treatment\": round(treat.mean(), 4),\n    \"mean_control\": round(control.mean(), 4)\n}\n\nregression_summary = model.summary()\n\nt_test_result, regression_summary\n\n({'t_statistic': 3.1014,\n  'p_value': 0.0019,\n  'mean_treatment': 0.022,\n  'mean_control': 0.0179},\n &lt;class 'statsmodels.iolib.summary.Summary'&gt;\n \"\"\"\n                             OLS Regression Results                            \n ==============================================================================\n Dep. Variable:                   gave   R-squared:                       0.000\n Model:                            OLS   Adj. R-squared:                  0.000\n Method:                 Least Squares   F-statistic:                     9.618\n Date:                Wed, 07 May 2025   Prob (F-statistic):            0.00193\n Time:                        20:24:31   Log-Likelihood:                 26630.\n No. Observations:               50083   AIC:                        -5.326e+04\n Df Residuals:                   50081   BIC:                        -5.324e+04\n Df Model:                           1                                         \n Covariance Type:            nonrobust                                         \n ==============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n ------------------------------------------------------------------------------\n const          0.0179      0.001     16.225      0.000       0.016       0.020\n treatment      0.0042      0.001      3.101      0.002       0.002       0.007\n ==============================================================================\n Omnibus:                    59814.280   Durbin-Watson:                   2.005\n Prob(Omnibus):                  0.000   Jarque-Bera (JB):          4317152.727\n Skew:                           6.740   Prob(JB):                         0.00\n Kurtosis:                      46.440   Cond. No.                         3.23\n ==============================================================================\n \n Notes:\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n \"\"\")\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_clean = df[['treatment', 'gave']].dropna()\n\nprobit_model = sm.Probit(df_clean['gave'], sm.add_constant(df_clean['treatment'])).fit(disp=False)\n\nsummary_stats = {\n    \"Pseudo R-squared\": probit_model.prsquared,\n    \"Log-Likelihood\": probit_model.llf,\n    \"AIC\": probit_model.aic,\n    \"BIC\": probit_model.bic,\n    \"Observations\": int(probit_model.nobs),\n    \"Coef (treatment)\": probit_model.params['treatment'],\n    \"Std Err (treatment)\": probit_model.bse['treatment'],\n    \"z-stat (treatment)\": probit_model.tvalues['treatment'],\n    \"P&gt;|z| (treatment)\": probit_model.pvalues['treatment'],\n    \"95% CI (treatment)\": probit_model.conf_int().loc['treatment'].tolist()\n}\n\nmarginal_effect = probit_model.get_margeff(at='overall').summary_frame().loc['treatment', 'dy/dx']\n\nsummary_stats[\"Marginal Effect (dy/dx)\"] = marginal_effect\n\nsummary_stats\n\n{'Pseudo R-squared': 0.0009782722838191926,\n 'Log-Likelihood': -5030.501164143209,\n 'AIC': 10065.002328286419,\n 'BIC': 10082.645202102685,\n 'Observations': 50083,\n 'Coef (treatment)': 0.08678462244745852,\n 'Std Err (treatment)': 0.027878757437573513,\n 'z-stat (treatment)': 3.1129300737949963,\n 'P&gt;|z| (treatment)': 0.001852399014778513,\n '95% CI (treatment)': [0.03214326193608627, 0.14142598295883077],\n 'Marginal Effect (dy/dx)': 0.004313211579633209}\n\n\nThis Probit model confirms that individuals who received a matched donation offer were significantly more likely to donate. The marginal effect suggests a 0.43 percentage point increase in the probability of donating—small but statistically significant (p &lt; 0.01).\n🧪 Statistical Approach & Interpretation\nIn this analysis, we test whether being assigned to the treatment group (which received a matched donation offer) significantly affects the likelihood of making a charitable donation.\nWe use two methods: - T-test: Compares the mean donation rates between treatment and control groups. - Bivariate Linear Regression: Regresses the binary outcome gave on the treatment variable to estimate the average treatment effect.\n📊 Key Results - The mean donation rate in the treatment group is higher than in the control group. - Both the t-test and regression confirm this difference is statistically significant at the 5% level. - The effect size is modest (a few tenths of a percentage point), but positive and consistent across methods.\n📌 Interpretation These results suggest that offering a matching donation increases the likelihood of giving. Even though the increase is small, it is statistically meaningful and aligns with behavioral insights: people respond to perceived value enhancement when their donation is matched.\nThis supports the idea that subtle psychological nudges—like matched giving—can effectively influence charitable behavior. The very low R-squared value is typical for binary outcome models with limited predictors and does not reduce the credibility of the treatment effect.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf['ratio_str'] = df['ratio'].replace({1: '1:1', 2: '2:1', 3: '3:1'})\n\nmatch_df = df[df['treatment'] == 1][['gave', 'ratio_str']].dropna()\n\ngave_1to1 = match_df[match_df['ratio_str'] == '1:1']['gave']\ngave_2to1 = match_df[match_df['ratio_str'] == '2:1']['gave']\ngave_3to1 = match_df[match_df['ratio_str'] == '3:1']['gave']\n\ntests = {\n    \"2:1 vs 1:1\": ttest_ind(gave_2to1, gave_1to1, nan_policy='omit'),\n    \"3:1 vs 1:1\": ttest_ind(gave_3to1, gave_1to1, nan_policy='omit'),\n    \"3:1 vs 2:1\": ttest_ind(gave_3to1, gave_2to1, nan_policy='omit')\n}\n\nprint(\"=== T-Test Results on Match Ratios ===\")\nfor label, result in tests.items():\n    t_stat, p_val = result\n    significance = \"✅ Significant\" if p_val &lt; 0.05 else \"❌ Not Significant\"\n    print(f\"{label:&lt;15} | t = {t_stat:.4f}, p = {p_val:.4f} | {significance}\")\n\n=== T-Test Results on Match Ratios ===\n2:1 vs 1:1      | t = 0.9650, p = 0.3345 | ❌ Not Significant\n3:1 vs 1:1      | t = 1.0150, p = 0.3101 | ❌ Not Significant\n3:1 vs 2:1      | t = 0.0501, p = 0.9600 | ❌ Not Significant\n\n\n/tmp/ipykernel_692213/955975056.py:5: FutureWarning:\n\nThe behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport pyreadstat\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_ratio = df[df['treatment'] == 1].copy()\n\ndf_ratio['ratio1'] = (df_ratio['ratio'] == 1).astype(int)\ndf_ratio['ratio2'] = (df_ratio['ratio'] == 2).astype(int)\ndf_ratio['ratio3'] = (df_ratio['ratio'] == 3).astype(int)\n\nX = df_ratio[['ratio2', 'ratio3']]  \nX = sm.add_constant(X)\ny = df_ratio['gave']\nmodel = sm.OLS(y, X).fit()\n\nregression_summary = model.summary()\nregression_summary\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.6454\n\n\nDate:\nWed, 07 May 2025\nProb (F-statistic):\n0.524\n\n\nTime:\n20:24:32\nLog-Likelihood:\n16688.\n\n\nNo. Observations:\n33396\nAIC:\n-3.337e+04\n\n\nDf Residuals:\n33393\nBIC:\n-3.334e+04\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.0207\n0.001\n14.912\n0.000\n0.018\n0.023\n\n\nratio2\n0.0019\n0.002\n0.958\n0.338\n-0.002\n0.006\n\n\nratio3\n0.0020\n0.002\n1.008\n0.313\n-0.002\n0.006\n\n\n\n\n\n\n\n\nOmnibus:\n38963.957\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2506478.937\n\n\nSkew:\n6.511\nProb(JB):\n0.00\n\n\nKurtosis:\n43.394\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression results show that neither the 2:1 nor the 3:1 match ratios led to a statistically significant increase in donation likelihood compared to the 1:1 match. While both ratio2 and ratio3 have positive coefficients, their p-values (0.338 and 0.313) are well above conventional significance thresholds. This suggests that offering a higher match doesn’t meaningfully change the probability that someone donates, at least in this context.\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf['ratio_str'] = df['ratio'].replace({1: '1:1', 2: '2:1', 3: '3:1'})\n\nmatch_df = df[df['treatment'] == 1][['gave', 'ratio_str']].dropna()\n\ngave_1to1 = match_df[match_df['ratio_str'] == '1:1']['gave']\ngave_2to1 = match_df[match_df['ratio_str'] == '2:1']['gave']\ngave_3to1 = match_df[match_df['ratio_str'] == '3:1']['gave']\n\nrate_1to1 = gave_1to1.mean()\nrate_2to1 = gave_2to1.mean()\nrate_3to1 = gave_3to1.mean()\n\ndiff_2_vs_1_data = rate_2to1 - rate_1to1\ndiff_3_vs_2_data = rate_3to1 - rate_2to1\n\ndf['ratio2'] = (df['ratio'] == 2).astype(int)\ndf['ratio3'] = (df['ratio'] == 3).astype(int)\nreg_df = df[df['treatment'] == 1][['gave', 'ratio2', 'ratio3']].dropna()\n\nimport statsmodels.api as sm\nX = sm.add_constant(reg_df[['ratio2', 'ratio3']])\ny = reg_df['gave']\nmodel = sm.OLS(y, X).fit()\n\ndiff_2_vs_1_reg = model.params['ratio2']\ndiff_3_vs_2_reg = model.params['ratio3'] - model.params['ratio2']\n\n{\n    \"Response Rate Difference (2:1 - 1:1) [Data]\": round(diff_2_vs_1_data, 4),\n    \"Response Rate Difference (3:1 - 2:1) [Data]\": round(diff_3_vs_2_data, 4),\n    \"Coefficient Difference (2:1 - 1:1) [Regression]\": round(diff_2_vs_1_reg, 4),\n    \"Coefficient Difference (3:1 - 2:1) [Regression]\": round(diff_3_vs_2_reg, 4)\n}\n\n/tmp/ipykernel_692213/1367793913.py:5: FutureWarning:\n\nThe behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n\n\n\n{'Response Rate Difference (2:1 - 1:1) [Data]': 0.0019,\n 'Response Rate Difference (3:1 - 2:1) [Data]': 0.0001,\n 'Coefficient Difference (2:1 - 1:1) [Regression]': 0.0019,\n 'Coefficient Difference (3:1 - 2:1) [Regression]': 0.0001}\n\n\nBased on both the direct data comparisons and the regression coefficients, the differences in response rates between match sizes are extremely small. The move from a 1:1 to a 2:1 match increases the donation rate by just 0.0019 (0.19 percentage points), and the jump from 2:1 to 3:1 adds only 0.0001 (0.01 percentage points). These effects are negligible in practice and statistically insignificant. The conclusion is clear: increasing the match ratio beyond 1:1 does not meaningfully improve donation rates. The mere presence of a match appears to matter, but making the match more generous does not lead to further increases in giving\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport pyreadstat\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_amount = df[['gave', 'amount', 'treatment']].dropna()\n\namount_treat = df_amount[df_amount['treatment'] == 1]['amount']\namount_control = df_amount[df_amount['treatment'] == 0]['amount']\nt_stat, p_value = ttest_ind(amount_treat, amount_control, nan_policy='omit')\n\nX = sm.add_constant(df_amount['treatment'])\ny = df_amount['amount']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_positive_donors = df_amount[df_amount['gave'] == 1].copy()\n\namount_treat = df_positive_donors[df_positive_donors['treatment'] == 1]['amount']\namount_control = df_positive_donors[df_positive_donors['treatment'] == 0]['amount']\nt_stat, p_value = ttest_ind(amount_treat, amount_control, nan_policy='omit')\n\nX = sm.add_constant(df_positive_donors['treatment'])\ny = df_positive_donors['amount']\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\ntreatment_donors = df_positive_donors[df_positive_donors['treatment'] == 1]['amount']\ncontrol_donors = df_positive_donors[df_positive_donors['treatment'] == 0]['amount']\n\nmean_treatment = treatment_donors.mean()\nmean_control = control_donors.mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\naxes[0].hist(treatment_donors, bins=30, color='skyblue', edgecolor='black')\naxes[0].axvline(mean_treatment, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = ${mean_treatment:.2f}\")\naxes[0].set_title(\"Treatment Group (Donors Only)\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\naxes[1].hist(control_donors, bins=30, color='lightgray', edgecolor='black')\naxes[1].axvline(mean_control, color='red', linestyle='dashed', linewidth=2, label=f\"Mean = ${mean_control:.2f}\")\naxes[1].set_title(\"Control Group (Donors Only)\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nprint(plt.show())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.461\nDate:                Wed, 07 May 2025   Prob (F-statistic):             0.0628\nTime:                        20:24:32   Log-Likelihood:            -1.7946e+05\nNo. Observations:               50083   AIC:                         3.589e+05\nDf Residuals:                   50081   BIC:                         3.589e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\nOmnibus:                    96861.113   Durbin-Watson:                   2.008\nProb(Omnibus):                  0.000   Jarque-Bera (JB):        240735713.635\nSkew:                          15.297   Prob(JB):                         0.00\nKurtosis:                     341.269   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 amount   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.001\nMethod:                 Least Squares   F-statistic:                    0.3374\nDate:                Wed, 07 May 2025   Prob (F-statistic):              0.561\nTime:                        20:24:32   Log-Likelihood:                -5326.8\nNo. Observations:                1034   AIC:                         1.066e+04\nDf Residuals:                    1032   BIC:                         1.067e+04\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\nOmnibus:                      587.258   Durbin-Watson:                   2.031\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             5623.279\nSkew:                           2.464   Prob(JB):                         0.00\nKurtosis:                      13.307   Cond. No.                         3.49\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nNone\n\n\nThe analysis shows that while the treatment group gave slightly more on average than the control group when everyone is included, the difference isn’t statistically significant at conventional levels. When we look only at those who actually donated, the control group gave more on average than the treatment group, and again, the difference isn’t significant. The histograms reinforce this, showing very similar distributions for both groups. This suggests that the match offer might help get more people to donate, but it doesn’t appear to influence how much they give once they’ve made the decision to contribute."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\ncontrol_draws = np.random.binomial(1, 0.018, size=10000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\ndiffs = treatment_draws - control_draws\n\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average of Differences')\nplt.axhline(0.004, color='red', linestyle='--', label='True Mean Difference (0.022 - 0.018)')\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.title(\"Law of Large Numbers: Convergence of Mean Difference\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot provides visual evidence of the Law of Large Numbers in action. Initially, the cumulative average of differences fluctuates significantly due to high variability in small sample comparisons. However, as the number of simulations increases, the cumulative average begins to stabilize and converge toward the true mean difference of 0.004 (i.e., 0.022 − 0.018), indicated by the red dashed line.\nBy the time we reach several thousand simulations, the cumulative average consistently hovers around the true difference, confirming that with enough observations, the sample mean reliably estimates the population mean difference. This reinforces the importance of large sample sizes when estimating small treatment effects.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nsample_sizes = [50, 200, 500, 1000]\ntrue_diff = 0.022 - 0.018\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(1000):\n        control_sample = np.random.binomial(1, 0.018, size=n)\n        treatment_sample = np.random.binomial(1, 0.022, size=n)\n        mean_diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(mean_diff)\n    \n    axes[i].hist(diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='black', linestyle='--')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label=f'True Diff = {true_diff:.4f}')\n    axes[i].set_title(f'Sample Size: {n}')\n    axes[i].legend()\n\nplt.suptitle(\"Central Limit Theorem: Sampling Distribution of Mean Differences\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn the histogram for sample size = 50, zero is approximately in the middle of the distribution, which means that with small samples, it’s difficult to distinguish whether the treatment has a meaningful effect—the distribution is too wide, and zero is a plausible average difference just by chance.\nAs the sample size increases to 200, 500, and especially 1000, zero moves toward the tails of the distribution. This suggests that with more data, the sample differences become more precise and center more tightly around the true mean difference (0.004), making it less likely that the observed effect is due to chance. At large sample sizes, zero is no longer a likely value, reinforcing confidence that the treatment has a small but real effect."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is project 2",
    "section": "",
    "text": "This is project 2"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nNivan Vora\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\nNivan Vora\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nNivan Vora\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nNivan Vora\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\ncustomers = df[df['iscustomer'] == 1]['patents']\nnon_customers = df[df['iscustomer'] == 0]['patents']\n\nplt.figure(figsize=(12, 6))\nplt.hist(non_customers, bins=30, alpha=0.5, label='Non-Customers', density=True, color='gold')\nplt.hist(customers, bins=30, alpha=0.5, label='Customers', density=True, color='coral')\nplt.title('Histogram of Number of Patents by Customer Status', fontsize=14)\nplt.xlabel('Number of Patents')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nmean_customers = customers.mean()\nmean_non_customers = non_customers.mean()\n\n(mean_customers, mean_non_customers)"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\ncustomers = df[df['iscustomer'] == 1]['patents']\nnon_customers = df[df['iscustomer'] == 0]['patents']\n\nplt.figure(figsize=(12, 6))\nplt.hist(non_customers, bins=30, alpha=0.5, label='Non-Customers', density=True, color='gold')\nplt.hist(customers, bins=30, alpha=0.5, label='Customers', density=True, color='coral')\nplt.title('Histogram of Number of Patents by Customer Status', fontsize=14)\nplt.xlabel('Number of Patents')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nmean_customers = customers.mean()\nmean_non_customers = non_customers.mean()\n\n(mean_customers, mean_non_customers)"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\nWe begin by examining the key variables that may influence the number of reviews, including price, room quality, and cleanliness.\nTo identify potential multicollinearity and relationships, we examine the correlation between numerical features.\n\n## Step 1: Data Cleaning\n\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ndf_clean = df.dropna(subset=[\n    'host_since', 'bathrooms', 'bedrooms',\n    'review_scores_cleanliness',\n    'review_scores_location',\n    'review_scores_value'\n])\nprint(f\"Original rows: {df.shape[0]}, After cleaning: {df_clean.shape[0]}\")\n\nOriginal rows: 40628, After cleaning: 30140\n\n\n\n## Step 2: Exploratory Data Analysis (EDA)\n\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ndf_clean = df.dropna(subset=[\n    'host_since', 'bathrooms', 'bedrooms',\n    'review_scores_cleanliness',\n    'review_scores_location',\n    'review_scores_value'\n])\n\nprint(df_clean[['price', 'number_of_reviews', 'review_scores_cleanliness',\n                'review_scores_location', 'review_scores_value']].describe())\n\nprint(df_clean[['price', 'days', 'bathrooms', 'bedrooms',\n                'review_scores_cleanliness', 'review_scores_location',\n                'review_scores_value', 'number_of_reviews']].corr())\n\n# Visualize distribution of number of reviews\nimport matplotlib.pyplot as plt\n\nplt.hist(df_clean['number_of_reviews'], bins=50)\nplt.title('Distribution of Number of Reviews')\nplt.xlabel('Number of Reviews')\nplt.ylabel('Frequency')\nplt.yscale('log') \nplt.show()\n\n              price  number_of_reviews  review_scores_cleanliness  \\\ncount  30140.000000       30140.000000               30140.000000   \nmean     140.211546          21.168115                   9.201758   \nstd      188.437967          32.004711                   1.114472   \nmin       10.000000           1.000000                   2.000000   \n25%       70.000000           3.000000                   9.000000   \n50%      103.000000           8.000000                  10.000000   \n75%      169.000000          26.000000                  10.000000   \nmax    10000.000000         421.000000                  10.000000   \n\n       review_scores_location  review_scores_value  \ncount            30140.000000         30140.000000  \nmean                 9.415428             9.334041  \nstd                  0.843181             0.900595  \nmin                  2.000000             2.000000  \n25%                  9.000000             9.000000  \n50%                 10.000000            10.000000  \n75%                 10.000000            10.000000  \nmax                 10.000000            10.000000  \n                              price      days  bathrooms  bedrooms  \\\nprice                      1.000000  0.038485   0.252537  0.292409   \ndays                       0.038485  1.000000  -0.014626  0.009249   \nbathrooms                  0.252537 -0.014626   1.000000  0.408606   \nbedrooms                   0.292409  0.009249   0.408606  1.000000   \nreview_scores_cleanliness  0.029101  0.017233   0.002770  0.002767   \nreview_scores_location     0.098469  0.023885  -0.029662 -0.049505   \nreview_scores_value        0.001771  0.006438   0.009287 -0.013171   \nnumber_of_reviews         -0.001728  0.217230  -0.014045  0.026733   \n\n                           review_scores_cleanliness  review_scores_location  \\\nprice                                       0.029101                0.098469   \ndays                                        0.017233                0.023885   \nbathrooms                                   0.002770               -0.029662   \nbedrooms                                    0.002767               -0.049505   \nreview_scores_cleanliness                   1.000000                0.327198   \nreview_scores_location                      0.327198                1.000000   \nreview_scores_value                         0.615623                0.448408   \nnumber_of_reviews                           0.029100               -0.049514   \n\n                           review_scores_value  number_of_reviews  \nprice                                 0.001771          -0.001728  \ndays                                  0.006438           0.217230  \nbathrooms                             0.009287          -0.014045  \nbedrooms                             -0.013171           0.026733  \nreview_scores_cleanliness             0.615623           0.029100  \nreview_scores_location                0.448408          -0.049514  \nreview_scores_value                   1.000000          -0.032310  \nnumber_of_reviews                    -0.032310           1.000000  \n\n\n\n\n\n\n\n\n\nThe number of reviews is highly skewed, with many listings receiving few reviews.\nSince number_of_reviews is a count variable, we model it using Poisson regression. We include key predictors such as price, duration on the platform, room type, review scores, and instant bookability.\n\n## Step 3: Poisson Regression Modeling\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport pandas as pd\ndf = pd.read_csv('airbnb.csv')\n\ndf_clean = df.dropna(subset=[\n    'host_since', 'bathrooms', 'bedrooms',\n    'review_scores_cleanliness',\n    'review_scores_location',\n    'review_scores_value'\n])\n\ndf_clean['instant_bookable'] = df_clean['instant_bookable'].map({'t': 1, 'f': 0})\n\nformula = \"number_of_reviews ~ price + days + bathrooms + bedrooms + review_scores_cleanliness + review_scores_location + review_scores_value + C(room_type) + instant_bookable\"\npoisson_model = smf.glm(formula=formula, data=df_clean, family=sm.families.Poisson()).fit()\n\nfrom IPython.display import display\nsummary_table = poisson_model.summary2().tables[1]\ndisplay(summary_table)\n\n/tmp/ipykernel_48660/3753103338.py:16: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nIntercept\n2.942697\n0.016633\n176.919813\n0.000000e+00\n2.910097\n2.975297\n\n\nC(room_type)[T.Private room]\n0.019233\n0.002738\n7.024594\n2.146901e-12\n0.013867\n0.024600\n\n\nC(room_type)[T.Shared room]\n-0.115193\n0.008650\n-13.317770\n1.824774e-40\n-0.132146\n-0.098240\n\n\nprice\n-0.000043\n0.000008\n-5.199251\n2.000937e-07\n-0.000059\n-0.000027\n\n\ndays\n0.000522\n0.000002\n280.429721\n0.000000e+00\n0.000518\n0.000525\n\n\nbathrooms\n-0.113444\n0.003773\n-30.066678\n1.321761e-198\n-0.120839\n-0.106049\n\n\nbedrooms\n0.075659\n0.002033\n37.214455\n3.983758e-303\n0.071675\n0.079644\n\n\nreview_scores_cleanliness\n0.110978\n0.001517\n73.158720\n0.000000e+00\n0.108005\n0.113952\n\n\nreview_scores_location\n-0.081481\n0.001617\n-50.402764\n0.000000e+00\n-0.084649\n-0.078313\n\n\nreview_scores_value\n-0.091055\n0.001847\n-49.310566\n0.000000e+00\n-0.094674\n-0.087436\n\n\ninstant_bookable\n0.459104\n0.002919\n157.293238\n0.000000e+00\n0.453383\n0.464824"
  },
  {
    "objectID": "blog/project 3/hw3_questions.html",
    "href": "blog/project 3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project 3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project 3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project 3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/project 3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport itertools\n\nnp.random.seed(123)\n\nbrand = [\"N\", \"P\", \"H\"]  \nad = [\"Yes\", \"No\"]\nprice = list(range(8, 33, 4))  \n\nprofiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=[\"brand\", \"ad\", \"price\"])\nm = len(profiles)\n\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\ndef sim_one(resp_id):\n    datlist = []\n\n    for t in range(1, n_tasks + 1):\n        sampled = profiles.sample(n=n_alts).copy()\n        sampled[\"resp\"] = resp_id\n        sampled[\"task\"] = t\n        sampled[\"v\"] = sampled[\"brand\"].map(b_util) + \\\n                       sampled[\"ad\"].map(a_util) + \\\n                       sampled[\"price\"].apply(p_util)\n        \n        gumbel_noise = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        sampled[\"u\"] = sampled[\"v\"] + gumbel_noise\n        \n        sampled[\"choice\"] = (sampled[\"u\"] == sampled[\"u\"].max()).astype(int)\n\n        datlist.append(sampled)\n\n    return pd.concat(datlist, ignore_index=True)\n\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nbrand\nad\nprice\nchoice\n\n\n\n\n0\n1\n1\nP\nNo\n32\n0\n\n\n1\n1\n1\nN\nNo\n28\n0\n\n\n2\n1\n1\nN\nNo\n24\n1\n\n\n3\n1\n2\nH\nNo\n28\n0\n\n\n4\n1\n2\nH\nNo\n8\n1"
  },
  {
    "objectID": "blog/project 3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/project 3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\nimport numpy as np\n\ndf_encoded = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\ndf_encoded.rename(columns={\n    \"brand_N\": \"is_netflix\",\n    \"brand_P\": \"is_prime\",\n    \"ad_Yes\": \"has_ads\"\n}, inplace=True)\n\nfeature_cols = [\"is_netflix\", \"is_prime\", \"has_ads\", \"price\"]\nX = df_encoded[feature_cols].astype(float).values\n\ny = df_encoded[\"choice\"].astype(int).values\n\ndf_encoded[\"task_id\"] = (\n    df_encoded[\"resp\"].astype(\"category\").cat.codes * 10 +\n    df_encoded[\"task\"]\n)\ngroup = df_encoded[\"task_id\"].values"
  },
  {
    "objectID": "blog/project 3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/project 3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nfrom scipy.special import logsumexp\nimport numpy as np\n\ndef mnl_log_likelihood(beta, X, y, group):\n    scores = X @ beta\n    log_like = 0.0\n\n    for task_id in np.unique(group):\n        task_mask = group == task_id\n        task_scores = scores[task_mask]\n        task_choices = y[task_mask]\n        log_probs = task_scores - logsumexp(task_scores)\n        log_like += np.sum(task_choices * log_probs)\n    return log_like\n\n\nfrom scipy.optimize import minimize\nfrom numpy.linalg import inv\nfrom scipy.stats import norm\nimport pandas as pd\nimport numdifftools as nd\nimport numpy as np\n\ndef neg_log_likelihood(b):\n    return -mnl_log_likelihood(b, X, y, group)\n\ninit_guess = np.zeros(X.shape[1])\nopt_result = minimize(neg_log_likelihood, init_guess, method=\"BFGS\")\nbeta_est = opt_result.x\n\nhessian_calc = nd.Hessian(lambda b: -mnl_log_likelihood(b, X, y, group))\nhessian_matrix = hessian_calc(beta_est)\nvcov_matrix = inv(hessian_matrix)\nse_est = np.sqrt(np.diag(vcov_matrix))\n\nz_score = norm.ppf(0.975)\nci_lo = beta_est - z_score * se_est\nci_hi = beta_est + z_score * se_est\n\ncoeff_labels = [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"]\nsummary_table = pd.DataFrame({\n    \"Parameter\": coeff_labels,\n    \"Estimate\": beta_est,\n    \"Std. Error\": se_est,\n    \"95% CI Lower\": ci_lo,\n    \"95% CI Upper\": ci_hi\n}).round(4)\n\nsummary_table\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nbeta_netflix\n0.6655\n0.0847\n0.4994\n0.8315\n\n\n1\nbeta_prime\n0.3289\n0.0878\n0.1569\n0.5009\n\n\n2\nbeta_ads\n-0.4749\n0.0682\n-0.6086\n-0.3412\n\n\n3\nbeta_price\n-0.0637\n0.0044\n-0.0723\n-0.0550\n\n\n\n\n\n\n\nExplanation of Maximum Likelihood Estimation Results\nTo estimate the Multinomial Logit (MNL) model, we first prepared the data by encoding categorical variables into binary indicators and constructing a feature matrix (X) capturing key attributes: streaming brand (Netflix, Prime, baseline Hulu), presence of ads, and price. We then defined a log-likelihood function that calculates the probability of each choice using the softmax of utilities derived from X @ beta, where beta represents the unknown preference weights.\nUsing the BFGS optimization method, we maximized this log-likelihood to find the Maximum Likelihood Estimates (MLE) of the beta parameters. To assess estimation precision, we computed standard errors from the inverse of the numerically approximated Hessian matrix and constructed 95% confidence intervals for each parameter.\nResults interpretation:\n\nThe estimated parameters reflect consumer preferences for different streaming service attributes.\n\nPositive coefficients for Netflix and Prime indicate that, all else equal, these brands are preferred over the baseline (Hulu), with Netflix receiving the highest utility.\nThe negative coefficient for ads confirms that consumers dislike advertisements, reducing the likelihood of choosing an ad-supported option.\nThe negative price coefficient reflects the expected inverse relationship between cost and choice probability—higher prices reduce utility.\n\nThe narrow confidence intervals across all parameters suggest that the estimates are statistically precise and support clear conclusions about consumer preferences."
  },
  {
    "objectID": "blog/project 3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/project 3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\n\nimport numpy as np\n\ndef log_prior(beta):\n    lp = 0\n    lp += -0.5 * np.sum((beta[:3] / 5) ** 2) - 3 * np.log(5 * np.sqrt(2 * np.pi))\n    lp += -0.5 * (beta[3] / 1) ** 2 - np.log(1 * np.sqrt(2 * np.pi))\n    return lp\n\ndef log_posterior(beta, X, y, group):\n    return mnl_log_likelihood(beta, X, y, group) + log_prior(beta)\n\nn_steps = 11000\nburn = 1000\nn_params = X.shape[1]\nsamples = np.zeros((n_steps, n_params))\naccepts = 0\n\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\ncurrent = np.zeros(n_params)\ncurrent_lp = log_posterior(current, X, y, group)\n\nrng = np.random.default_rng(123)\nfor t in range(n_steps):\n    proposal = current + rng.normal(0, proposal_sd)\n    proposal_lp = log_posterior(proposal, X, y, group)\n    log_accept_ratio = proposal_lp - current_lp\n    if np.log(rng.uniform()) &lt; log_accept_ratio:\n        current = proposal\n        current_lp = proposal_lp\n        accepts += 1\n    samples[t] = current\nposterior_samples = samples[burn:]\n\n\nimport matplotlib.pyplot as plt\n\nparam_names = [\"beta_netflix\", \"beta_ads\", \"beta_price\"]\nparam_indices = [0, 2, 3]\n\nplt.figure(figsize=(15, 9))\nfor i, (name, idx) in enumerate(zip(param_names, param_indices)):\n    plt.subplot(3, 2, 2*i+1)\n    plt.plot(posterior_samples[:, idx])\n    plt.title(f\"Trace plot: {name}\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Value\")\n    \n    plt.subplot(3, 2, 2*i+2)\n    plt.hist(posterior_samples[:, idx], bins=40, density=True)\n    plt.title(f\"Posterior: {name}\")\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nmeans = posterior_samples.mean(axis=0)\nstds = posterior_samples.std(axis=0)\nci_los = np.percentile(posterior_samples, 2.5, axis=0)\nci_his = np.percentile(posterior_samples, 97.5, axis=0)\n\nsummary = pd.DataFrame({\n    \"Parameter\": [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"],\n    \"Posterior Mean\": means,\n    \"Posterior SD\": stds,\n    \"95% CI Lower\": ci_los,\n    \"95% CI Upper\": ci_his\n}).round(4)\n\nprint(\"Acceptance rate:\", accepts / n_steps)\nsummary\n\nAcceptance rate: 0.45436363636363636\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior SD\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nbeta_netflix\n0.6679\n0.0828\n0.5109\n0.8366\n\n\n1\nbeta_prime\n0.3328\n0.0892\n0.1572\n0.5062\n\n\n2\nbeta_ads\n-0.4728\n0.0680\n-0.6095\n-0.3386\n\n\n3\nbeta_price\n-0.0639\n0.0044\n-0.0724\n-0.0552\n\n\n\n\n\n\n\n\ndef ci_bracket(lo, hi):\n    return [f\"[{l:.3f}, {h:.3f}]\" for l, h in zip(lo, hi)]\n\ncomparison = pd.DataFrame({\n    \"Parameter\": [\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"],\n    \"MLE Mean (SE)\": summary_table[\"Estimate\"].round(3).astype(str) + \" (\" + summary_table[\"Std. Error\"].round(3).astype(str) + \")\",\n    \"MLE 95% CI\": ci_bracket(summary_table[\"95% CI Lower\"], summary_table[\"95% CI Upper\"]),\n    \"Bayes Mean (SD)\": summary[\"Posterior Mean\"].round(3).astype(str) + \" (\" + summary[\"Posterior SD\"].round(3).astype(str) + \")\",\n    \"Bayes 95% CI\": ci_bracket(summary[\"95% CI Lower\"], summary[\"95% CI Upper\"])\n})\ncomparison\n\n\n\n\n\n\n\n\nParameter\nMLE Mean (SE)\nMLE 95% CI\nBayes Mean (SD)\nBayes 95% CI\n\n\n\n\n0\nbeta_netflix\n0.666 (0.085)\n[0.499, 0.832]\n0.668 (0.083)\n[0.511, 0.837]\n\n\n1\nbeta_prime\n0.329 (0.088)\n[0.157, 0.501]\n0.333 (0.089)\n[0.157, 0.506]\n\n\n2\nbeta_ads\n-0.475 (0.068)\n[-0.609, -0.341]\n-0.473 (0.068)\n[-0.610, -0.339]\n\n\n3\nbeta_price\n-0.064 (0.004)\n[-0.072, -0.055]\n-0.064 (0.004)\n[-0.072, -0.055]"
  },
  {
    "objectID": "blog/project 3/hw3_questions.html#discussion",
    "href": "blog/project 3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nIf the data were not simulated (i.e., if we used real-world data), the parameter estimates would reflect actual consumer preferences rather than the “true” values we set in the simulation. Here’s what this means in practice:\n\nParameter Estimates and Real Data:\n\nWith real-world data, the estimated parameters (like \\(\\beta_\\text{Netflix}\\), \\(\\beta_\\text{Prime}\\), and \\(\\beta_\\text{price}\\)) are influenced by:\n\nSampling variability: Differences that arise just by chance because we only observe a sample, not the entire population.\nUnobserved heterogeneity: Variation in preferences that the model does not capture (e.g., individual tastes or external factors).\nModel misspecification: If the model does not accurately represent the real-world decision process, estimates may be biased.\n\n\nInterpreting Coefficients:\n\n\\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\):\n\nImplies that, on average, consumers derive more utility from Netflix than from Prime, holding other factors constant.\nInterpretation: Netflix is preferred over Prime by consumers.\n\n\\(\\beta_\\text{price} &lt; 0\\) (Negative Price Coefficient):\n\nIndicates that higher prices decrease the likelihood of a product being chosen.\nEconomic intuition: Consumers generally prefer lower prices, so this result is expected.\n\n\\(\\beta_\\text{price} &gt; 0\\) (Positive Price Coefficient):\n\nWould suggest that higher prices increase utility, which is counterintuitive.\nAction: This would prompt further investigation into the data or model, as it may indicate issues such as data errors or incorrect model specification.\n\n\nSummary:\n\nIn real-world applications, parameter estimates help us understand actual consumer behavior.\nThe sign and magnitude of each coefficient provide insights into preferences and sensitivities to different product attributes.\n\n\nTo simulate and estimate a multi-level (random-parameter or hierarchical) multinomial logit model, you need to allow the preference parameters (betas) to vary across individuals rather than being fixed for everyone.\nSimulation:\n- Instead of using a single set of betas for all respondents, draw a unique beta vector for each respondent from a population distribution (e.g., multivariate normal with mean vector μ and covariance Σ). - For each respondent, simulate choices using their individual beta.\nEstimation:\n- The likelihood must integrate over the distribution of individual-level betas, or you must explicitly estimate both the population-level parameters (μ, Σ) and the individual-level betas. - In a Bayesian framework, use hierarchical priors:\n- Place priors on μ and Σ, and sample individual betas as latent variables. - Use MCMC methods (e.g., Gibbs sampling, hierarchical Metropolis-Hastings) to jointly sample μ, Σ, and all individual betas. - In a frequentist framework, use methods like simulated maximum likelihood or the Expectation-Maximization (EM) algorithm.\nSummary:\n- The key change is modeling individual heterogeneity by introducing a distribution over betas, and updating both the population and individual-level parameters during estimation."
  },
  {
    "objectID": "blog/project 4/hw4_questions.html",
    "href": "blog/project 4/hw4_questions.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This project explores several key techniques in marketing analytics using real-world and synthetic datasets. The main datasets used are:\n\n\nThe Palmer Penguins dataset contains measurements for three species of penguins (Adelie, Chinstrap, and Gentoo) collected from islands in the Palmer Archipelago, Antarctica. Key variables include bill length, bill depth, flipper length, body mass, and species. This dataset is commonly used as an alternative to the Iris dataset for demonstrating clustering and classification algorithms.\nIn this project:\nWe use the Palmer Penguins dataset to implement and visualize the k-means clustering algorithm from scratch, compare it to built-in implementations, and evaluate clustering quality using metrics like within-cluster sum of squares and silhouette scores.\n\n\n\nThe drivers analysis dataset contains survey responses measuring customer satisfaction and various perceptions or experiences (such as service quality, value, etc.), along with possible identifiers like brand or respondent ID. The goal is to understand which factors are most important in driving customer satisfaction.\nIn this project:\nWe apply a variety of statistical and machine learning methods to assess the importance of each predictor variable in explaining satisfaction. Methods include correlation analysis, regression coefficients, Shapley values, Johnson’s relative weights, and feature importances from random forest, XGBoost, and neural networks.\n\n\n\n\nClustering (K-Means): Implement k-means from scratch, visualize the algorithm’s steps, compare with scikit-learn, and determine the optimal number of clusters.\nKey Drivers Analysis: Quantify the importance of predictors for customer satisfaction using multiple statistical and machine learning approaches, and summarize results in a comparative table."
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#a.-k-means",
    "href": "blog/project 4/hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    # Load data\n    df = pd.read_csv('palmer_penguins.csv')\n    data = df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n    def initialize_centroids(X, k):\n        idx = np.random.choice(len(X), k, replace=False)\n        return X[idx]\n\n    def assign_clusters(X, centroids):\n        dists = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        return np.argmin(dists, axis=1)\n\n    def update_centroids(X, labels, k):\n        return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\n    def kmeans(X, k, max_iters=10, plot_steps=True):\n        centroids = initialize_centroids(X, k)\n        for it in range(max_iters):\n            labels = assign_clusters(X, centroids)\n            new_centroids = update_centroids(X, labels, k)\n            if plot_steps:\n                plt.figure(figsize=(6,4))\n                for i in range(k):\n                    plt.scatter(X[labels==i,0], X[labels==i,1], label=f'Cluster {i+1}')\n                plt.scatter(centroids[:,0], centroids[:,1], c='black', marker='x', s=100, label='Centroids')\n                plt.title(f'Iteration {it+1}')\n                plt.xlabel('Bill Length (mm)')\n                plt.ylabel('Flipper Length (mm)')\n                plt.legend()\n                plt.show()\n            if np.allclose(centroids, new_centroids):\n                break\n            centroids = new_centroids\n        return labels, centroids\n\n    # Run custom k-means\n    np.random.seed(42)\n    k = 3\n    labels, centroids = kmeans(data, k, max_iters=10, plot_steps=True)\n\n    # Compare with scikit-learn\n    from sklearn.cluster import KMeans\n\n    kmeans_builtin = KMeans(n_clusters=k, random_state=42)\n    labels_builtin = kmeans_builtin.fit_predict(data)\n\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    for i in range(k):\n        plt.scatter(data[labels==i,0], data[labels==i,1], label=f'Cluster {i+1}')\n    plt.scatter(centroids[:,0], centroids[:,1], c='black', marker='x', s=100, label='Centroids')\n    plt.title('Custom K-Means')\n    plt.xlabel('Bill Length (mm)')\n    plt.ylabel('Flipper Length (mm)')\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    for i in range(k):\n        plt.scatter(data[labels_builtin==i,0], data[labels_builtin==i,1], label=f'Cluster {i+1}')\n    plt.scatter(kmeans_builtin.cluster_centers_[:,0], kmeans_builtin.cluster_centers_[:,1], c='black', marker='x', s=100, label='Centroids')\n    plt.title('scikit-learn KMeans')\n    plt.xlabel('Bill Length (mm)')\n    plt.ylabel('Flipper Length (mm)')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom K-Means Implementation and Comparison\nWe implemented the k-means clustering algorithm from scratch in Python, visualizing each iteration to observe how the centroids and cluster assignments evolve. The algorithm was tested on the Palmer Penguins dataset using the bill_length_mm and flipper_length_mm features. For comparison, we also applied the built-in KMeans function from scikit-learn.\n\nStep-by-step plots: At each iteration, we plotted the data points colored by their current cluster assignment, along with the current centroid locations. This allowed us to visually track the convergence of the algorithm.\nComparison: After running our custom implementation, we compared the final cluster assignments and centroids to those produced by scikit-learn’s KMeans. The results were visually and numerically similar, confirming the correctness of our implementation.\nAnimated GIF: To further illustrate the clustering process, we generated an animated GIF showing the progression of the algorithm over iterations.\n\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\n\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsil_scores = []\nK_range = range(2, 8)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.flatten()\n\nfor idx, k in enumerate(K_range):\n    kmeans_model = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans_model.fit_predict(data)\n    wcss.append(kmeans_model.inertia_)\n    sil = silhouette_score(data, labels)\n    sil_scores.append(sil)\n    \n    # Plot clusters for each k\n    ax = axes[idx]\n    for i in range(k):\n        ax.scatter(data[labels==i,0], data[labels==i,1], label=f'Cluster {i+1}')\n    ax.scatter(kmeans_model.cluster_centers_[:,0], kmeans_model.cluster_centers_[:,1], c='black', marker='x', s=100, label='Centroids')\n    ax.set_title(f'K={k}')\n    ax.set_xlabel('Bill Length (mm)')\n    ax.set_ylabel('Flipper Length (mm)')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(K_range, wcss, marker='o')\nplt.title('Within-Cluster Sum of Squares (WCSS)')\nplt.xlabel('Number of clusters (K)')\nplt.ylabel('WCSS')\n\nplt.subplot(1,2,2)\nplt.plot(K_range, sil_scores, marker='o')\nplt.title('Silhouette Score')\nplt.xlabel('Number of clusters (K)')\nplt.ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\nbest_k_wcss = K_range[wcss.index(min(wcss))]\nbest_k_sil = K_range[sil_scores.index(max(sil_scores))]\nprint(f\"Lowest WCSS at K={best_k_wcss}, highest silhouette score at K={best_k_sil}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLowest WCSS at K=7, highest silhouette score at K=2\n\n\nRight number of Clusters:\nTo determine the “right” number of clusters, we examine both the within-cluster-sum-of-squares (WCSS) and the silhouette score across different values of K (number of clusters):\n\nWithin-Cluster-Sum-of-Squares (WCSS):\nWCSS measures the total squared distance between each point and its assigned cluster centroid. As K increases, WCSS always decreases, but the rate of decrease slows down. The “elbow” method suggests choosing K at the point where adding another cluster does not significantly reduce WCSS. In the plot, there is a noticeable elbow at K=3, indicating that increasing beyond 3 clusters yields only marginal improvement in compactness.\nSilhouette Score:\nThe silhouette score measures how similar each point is to its own cluster compared to other clusters, with higher values indicating better-defined clusters. In the plot, the silhouette score peaks at K=2, suggesting that the data is best separated into 2 clusters.\n\nConclusion:\n- The WCSS “elbow” method suggests K=3 as a reasonable choice. - The silhouette score suggests K=2 as the optimal number of clusters.\nTherefore, the “right” number of clusters depends on the metric: - K=2 according to the silhouette score (best separation). - K=3 according to the WCSS elbow method (balance between compactness and simplicity).\nIn practice, you may consider the context of your data and the interpretability of the clusters when making the final choice.\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this.\n\nimport imageio\nimport os\n\ndef kmeans_gif(X, k, max_iters=10, gif_path='kmeans_animation.gif'):\n    centroids = initialize_centroids(X, k)\n    images = []\n    for it in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        fig, ax = plt.subplots(figsize=(6,4))\n        colors = ['red', 'gold', 'magenta', 'blue', 'green', 'cyan', 'orange']\n        for i in range(k):\n            ax.scatter(X[labels==i,0], X[labels==i,1], color=colors[i%len(colors)], s=10)\n            ax.scatter(centroids[i,0], centroids[i,1], color=colors[i%len(colors)], edgecolor='black', s=100, marker='o', linewidth=2)\n        ax.set_title(f'Iteration {it+1}')\n        ax.set_xlabel('Bill Length (mm)')\n        ax.set_ylabel('Flipper Length (mm)')\n        ax.set_xlim(X[:,0].min()-1, X[:,0].max()+1)\n        ax.set_ylim(X[:,1].min()-5, X[:,1].max()+5)\n        fname = f'_kmeans_step_{it}.png'\n        fig.savefig(fname)\n        plt.close(fig)\n        images.append(imageio.imread(fname))\n        os.remove(fname)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    imageio.mimsave(gif_path, images, duration=0.8)\n    print(f\"Animated GIF saved to {gif_path}\")\n\n# Run and save GIF\nnp.random.seed(42)\nkmeans_gif(data, k=3, max_iters=10, gif_path='kmeans_animation.gif')\n\n/tmp/ipykernel_52184/1954122467.py:23: DeprecationWarning:\n\nStarting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n\n\n\nAnimated GIF saved to kmeans_animation.gif"
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#b.-latent-class-mnl",
    "href": "blog/project 4/hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "blog/project 4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#b.-key-drivers-analysis",
    "href": "blog/project 4/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#project-overview",
    "href": "blog/project 4/hw4_questions.html#project-overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This project explores several key techniques in marketing analytics using real-world and synthetic datasets. The main datasets used are:\n\n\nThe Palmer Penguins dataset contains measurements for three species of penguins (Adelie, Chinstrap, and Gentoo) collected from islands in the Palmer Archipelago, Antarctica. Key variables include bill length, bill depth, flipper length, body mass, and species. This dataset is commonly used as an alternative to the Iris dataset for demonstrating clustering and classification algorithms.\nIn this project:\nWe use the Palmer Penguins dataset to implement and visualize the k-means clustering algorithm from scratch, compare it to built-in implementations, and evaluate clustering quality using metrics like within-cluster sum of squares and silhouette scores.\n\n\n\nThe drivers analysis dataset contains survey responses measuring customer satisfaction and various perceptions or experiences (such as service quality, value, etc.), along with possible identifiers like brand or respondent ID. The goal is to understand which factors are most important in driving customer satisfaction.\nIn this project:\nWe apply a variety of statistical and machine learning methods to assess the importance of each predictor variable in explaining satisfaction. Methods include correlation analysis, regression coefficients, Shapley values, Johnson’s relative weights, and feature importances from random forest, XGBoost, and neural networks.\n\n\n\n\nClustering (K-Means): Implement k-means from scratch, visualize the algorithm’s steps, compare with scikit-learn, and determine the optimal number of clusters.\nKey Drivers Analysis: Quantify the importance of predictors for customer satisfaction using multiple statistical and machine learning approaches, and summarize results in a comparative table."
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#k-means",
    "href": "blog/project 4/hw4_questions.html#k-means",
    "title": "Machine Learning",
    "section": "1. K-Means",
    "text": "1. K-Means\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    # Load data\n    df = pd.read_csv('palmer_penguins.csv')\n    data = df[['bill_length_mm', 'flipper_length_mm']].dropna().values\n\n    def initialize_centroids(X, k):\n        idx = np.random.choice(len(X), k, replace=False)\n        return X[idx]\n\n    def assign_clusters(X, centroids):\n        dists = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        return np.argmin(dists, axis=1)\n\n    def update_centroids(X, labels, k):\n        return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\n    def kmeans(X, k, max_iters=10, plot_steps=True):\n        centroids = initialize_centroids(X, k)\n        for it in range(max_iters):\n            labels = assign_clusters(X, centroids)\n            new_centroids = update_centroids(X, labels, k)\n            if plot_steps:\n                plt.figure(figsize=(6,4))\n                for i in range(k):\n                    plt.scatter(X[labels==i,0], X[labels==i,1], label=f'Cluster {i+1}')\n                plt.scatter(centroids[:,0], centroids[:,1], c='black', marker='x', s=100, label='Centroids')\n                plt.title(f'Iteration {it+1}')\n                plt.xlabel('Bill Length (mm)')\n                plt.ylabel('Flipper Length (mm)')\n                plt.legend()\n                plt.show()\n            if np.allclose(centroids, new_centroids):\n                break\n            centroids = new_centroids\n        return labels, centroids\n\n    # Run custom k-means\n    np.random.seed(42)\n    k = 3\n    labels, centroids = kmeans(data, k, max_iters=10, plot_steps=True)\n\n    # Compare with scikit-learn\n    from sklearn.cluster import KMeans\n\n    kmeans_builtin = KMeans(n_clusters=k, random_state=42)\n    labels_builtin = kmeans_builtin.fit_predict(data)\n\n    plt.figure(figsize=(12,5))\n    plt.subplot(1,2,1)\n    for i in range(k):\n        plt.scatter(data[labels==i,0], data[labels==i,1], label=f'Cluster {i+1}')\n    plt.scatter(centroids[:,0], centroids[:,1], c='black', marker='x', s=100, label='Centroids')\n    plt.title('Custom K-Means')\n    plt.xlabel('Bill Length (mm)')\n    plt.ylabel('Flipper Length (mm)')\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    for i in range(k):\n        plt.scatter(data[labels_builtin==i,0], data[labels_builtin==i,1], label=f'Cluster {i+1}')\n    plt.scatter(kmeans_builtin.cluster_centers_[:,0], kmeans_builtin.cluster_centers_[:,1], c='black', marker='x', s=100, label='Centroids')\n    plt.title('scikit-learn KMeans')\n    plt.xlabel('Bill Length (mm)')\n    plt.ylabel('Flipper Length (mm)')\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom K-Means Implementation and Comparison\nWe implemented the k-means clustering algorithm from scratch in Python, visualizing each iteration to observe how the centroids and cluster assignments evolve. The algorithm was tested on the Palmer Penguins dataset using the bill_length_mm and flipper_length_mm features. For comparison, we also applied the built-in KMeans function from scikit-learn.\n\nStep-by-step plots: At each iteration, we plotted the data points colored by their current cluster assignment, along with the current centroid locations. This allowed us to visually track the convergence of the algorithm.\nComparison: After running our custom implementation, we compared the final cluster assignments and centroids to those produced by scikit-learn’s KMeans. The results were visually and numerically similar, confirming the correctness of our implementation.\nAnimated GIF: To further illustrate the clustering process, we generated an animated GIF showing the progression of the algorithm over iterations.\n\n\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsil_scores = []\nK_range = range(2, 8)\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.flatten()\n\nfor idx, k in enumerate(K_range):\n    kmeans_model = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans_model.fit_predict(data)\n    wcss.append(kmeans_model.inertia_)\n    sil = silhouette_score(data, labels)\n    sil_scores.append(sil)\n    \n    # Plot clusters for each k\n    ax = axes[idx]\n    for i in range(k):\n        ax.scatter(data[labels==i,0], data[labels==i,1], label=f'Cluster {i+1}')\n    ax.scatter(kmeans_model.cluster_centers_[:,0], kmeans_model.cluster_centers_[:,1], c='black', marker='x', s=100, label='Centroids')\n    ax.set_title(f'K={k}')\n    ax.set_xlabel('Bill Length (mm)')\n    ax.set_ylabel('Flipper Length (mm)')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplt.plot(K_range, wcss, marker='o')\nplt.title('Within-Cluster Sum of Squares (WCSS)')\nplt.xlabel('Number of clusters (K)')\nplt.ylabel('WCSS')\n\nplt.subplot(1,2,2)\nplt.plot(K_range, sil_scores, marker='o')\nplt.title('Silhouette Score')\nplt.xlabel('Number of clusters (K)')\nplt.ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\nbest_k_wcss = K_range[wcss.index(min(wcss))]\nbest_k_sil = K_range[sil_scores.index(max(sil_scores))]\nprint(f\"Lowest WCSS at K={best_k_wcss}, highest silhouette score at K={best_k_sil}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLowest WCSS at K=7, highest silhouette score at K=2\n\n\nRight number of Clusters:\nTo determine the “right” number of clusters, we examine both the within-cluster-sum-of-squares (WCSS) and the silhouette score across different values of K (number of clusters):\n\nWithin-Cluster-Sum-of-Squares (WCSS):\nWCSS measures the total squared distance between each point and its assigned cluster centroid. As K increases, WCSS always decreases, but the rate of decrease slows down. The “elbow” method suggests choosing K at the point where adding another cluster does not significantly reduce WCSS. In the plot, there is a noticeable elbow at K=3, indicating that increasing beyond 3 clusters yields only marginal improvement in compactness.\nSilhouette Score:\nThe silhouette score measures how similar each point is to its own cluster compared to other clusters, with higher values indicating better-defined clusters. In the plot, the silhouette score peaks at K=2, suggesting that the data is best separated into 2 clusters.\n\nConclusion:\n- The WCSS “elbow” method suggests K=3 as a reasonable choice. - The silhouette score suggests K=2 as the optimal number of clusters.\nTherefore, the “right” number of clusters depends on the metric: - K=2 according to the silhouette score (best separation). - K=3 according to the WCSS elbow method (balance between compactness and simplicity).\nIn practice, you may consider the context of your data and the interpretability of the clusters when making the final choice.\nChallenge:\nAs an extra step, we created an animated GIF to visually demonstrate how the k-means algorithm iteratively updates cluster assignments and centroids. This helps illustrate the convergence process and makes the clustering steps easy to follow.\n\nimport imageio\nimport os\n\ndef kmeans_gif(X, k, max_iters=10, gif_path='kmeans_animation.gif'):\n    centroids = initialize_centroids(X, k)\n    images = []\n    for it in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        fig, ax = plt.subplots(figsize=(6,4))\n        colors = ['red', 'gold', 'magenta', 'blue', 'green', 'cyan', 'orange']\n        for i in range(k):\n            ax.scatter(X[labels==i,0], X[labels==i,1], color=colors[i%len(colors)], s=10)\n            ax.scatter(centroids[i,0], centroids[i,1], color=colors[i%len(colors)], edgecolor='black', s=100, marker='o', linewidth=2)\n        ax.set_title(f'Iteration {it+1}')\n        ax.set_xlabel('Bill Length (mm)')\n        ax.set_ylabel('Flipper Length (mm)')\n        ax.set_xlim(X[:,0].min()-1, X[:,0].max()+1)\n        ax.set_ylim(X[:,1].min()-5, X[:,1].max()+5)\n        fname = f'_kmeans_step_{it}.png'\n        fig.savefig(fname)\n        plt.close(fig)\n        images.append(imageio.imread(fname))\n        os.remove(fname)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    imageio.mimsave(gif_path, images, duration=0.8)\n    print(f\"Animated GIF saved to {gif_path}\")\n\n# Run and save GIF\nnp.random.seed(42)\nkmeans_gif(data, k=3, max_iters=10, gif_path='kmeans_animation.gif')\n\n/tmp/ipykernel_76185/1954122467.py:23: DeprecationWarning:\n\nStarting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n\n\n\nAnimated GIF saved to kmeans_animation.gif"
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#key-drivers-analysis",
    "href": "blog/project 4/hw4_questions.html#key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2. Key Drivers Analysis",
    "text": "2. Key Drivers Analysis\n\nMethods and Results\nIn this section, we created the key drivers analysis table using the drivers analysis dataset. The following methods were applied to assess the importance of each predictor variable in explaining customer satisfaction:\n\nPearson Correlations: Measures the linear relationship between each predictor and satisfaction.\nPolychoric Correlations (approximated with Spearman): Measures monotonic relationships, useful for ordinal or non-linear associations.\nStandardized Regression Coefficients: Obtained from a linear regression with standardized predictors, indicating the relative effect size of each variable.\nUsefulness: The increase in R² when each variable is added last to the regression model, showing its unique contribution.\nLMG / Shapley Values: Decomposes the model’s R² into contributions from each predictor, accounting for shared variance.\nJohnson’s Epsilon (Relative Weights): Approximated using random forest feature importances, reflecting the relative importance of predictors.\nMean Decrease in RF Gini Coefficient: Measures the importance of each variable in a random forest model based on the reduction in node impurity.\n\nThe table below summarizes the results for each method:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom scipy.stats import pearsonr\nimport shap\n\n# Load data\ndf = pd.read_csv('data_for_drivers_analysis.csv')\n\n# Exclude 'brand' and 'id' from predictors if present\nexclude_cols = ['brand', 'id']\npredictors = [col for col in df.columns if col not in exclude_cols + ['satisfaction']]\nX = df[predictors]\ny = df['satisfaction']\n\n# Standardize predictors\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\nX_std_df = pd.DataFrame(X_std, columns=predictors)\n\n# 1. Pearson correlations\npearson_corrs = [pearsonr(X[col], y)[0] for col in predictors]\n\n# 2. Polychoric correlations (approximate with Spearman if not ordinal)\nspearman_corrs = [X[col].corr(y, method='spearman') for col in predictors]\n\n# 3. Standardized regression coefficients\nlr = LinearRegression()\nlr.fit(X_std, y)\nstd_coefs = lr.coef_\n\n# 4. \"Usefulness\" (increase in R^2 when adding each variable last)\ndef usefulness(X, y):\n    usefulness_scores = []\n    for col in X.columns:\n        X_other = X.drop(col, axis=1)\n        lr.fit(X_other, y)\n        r2_without = r2_score(y, lr.predict(X_other))\n        lr.fit(X, y)\n        r2_with = r2_score(y, lr.predict(X))\n        usefulness_scores.append(r2_with - r2_without)\n    return usefulness_scores\n\nusefulness_scores = usefulness(X_std_df, y)\n\n# 5. Shapley values (LMG)\nexplainer = shap.Explainer(lr, X_std)\nshap_values = explainer(X_std)\nshap_means = np.abs(shap_values.values).mean(axis=0)\n\n# 6. Johnson's relative weights (approximate via random forest feature importances)\ndef johnson_relative_weights(X, y):\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n    rf.fit(X, y)\n    return rf.feature_importances_\n\njohnson_weights = johnson_relative_weights(X_std, y)\n\n# 7. Mean decrease in Gini coefficient (from random forest)\n# 7. Mean decrease in Gini coefficient (from random forest)\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X, y)\ngini_importances = rf.feature_importances_\n\n# Combine results into a DataFrame\nresults = pd.DataFrame({\n    'Perception': predictors,\n    'Pearson Correlations': np.round(pearson_corrs, 3),\n    'Polychoric Correlations': np.round(spearman_corrs, 3),\n    'Standardized Regression Coefficients': np.round(std_coefs, 3),\n    'Usefulness': np.round(usefulness_scores, 3),\n    'LMG / Shapley Values': np.round(shap_means / shap_means.sum(), 3),\n    \"Johnson's Epsilon\": np.round(johnson_weights / johnson_weights.sum(), 3),\n    'Mean Decrease in RF Gini Coefficient': np.round(gini_importances / gini_importances.sum(), 3)\n})\n\n# Display table\nresults\n\n\n\n\n\n\n\n\nPerception\nPearson Correlations\nPolychoric Correlations\nStandardized Regression Coefficients\nUsefulness\nLMG / Shapley Values\nJohnson's Epsilon\nMean Decrease in RF Gini Coefficient\n\n\n\n\n0\ntrust\n0.256\n0.253\n0.136\n0.008\n0.267\n0.156\n0.156\n\n\n1\nbuild\n0.192\n0.195\n0.023\n0.000\n0.045\n0.102\n0.102\n\n\n2\ndiffers\n0.185\n0.190\n0.033\n0.001\n0.056\n0.090\n0.090\n\n\n3\neasy\n0.213\n0.212\n0.026\n0.000\n0.051\n0.100\n0.100\n\n\n4\nappealing\n0.208\n0.204\n0.040\n0.001\n0.076\n0.086\n0.086\n\n\n5\nrewarding\n0.195\n0.199\n0.006\n0.000\n0.011\n0.101\n0.101\n\n\n6\npopular\n0.171\n0.171\n0.019\n0.000\n0.038\n0.095\n0.095\n\n\n7\nservice\n0.251\n0.252\n0.104\n0.005\n0.199\n0.130\n0.130\n\n\n8\nimpact\n0.255\n0.261\n0.150\n0.011\n0.255\n0.141\n0.141\n\n\n\n\n\n\n\nAdditional Measures:\nWe extended the analysis by including variable importance scores from XGBoost and permutation importance from a neural network (MLP). These methods provide further perspectives on predictor relevance using advanced machine learning models.\n\nfrom xgboost import XGBRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n# XGBoost feature importances\nxgb = XGBRegressor(n_estimators=100, random_state=42)\nxgb.fit(X, y)\nxgb_importances = xgb.feature_importances_\nxgb_importances_norm = xgb_importances / xgb_importances.sum()\n\n# Neural Network feature importances (permutation importance)\nfrom sklearn.inspection import permutation_importance\nmlp = MLPRegressor(hidden_layer_sizes=(32, 16), max_iter=1000, random_state=42)\nmlp.fit(X_std, y)\nperm_importance = permutation_importance(mlp, X_std, y, n_repeats=10, random_state=42)\nnn_importances = perm_importance.importances_mean\nnn_importances_norm = nn_importances / nn_importances.sum()\n\n# Add to results table\nresults['XGBoost Importance'] = np.round(xgb_importances_norm, 3)\nresults['Neural Net Permutation Importance'] = np.round(nn_importances_norm, 3)\n\nresults\n\n\n\n\n\n\n\n\nPerception\nPearson Correlations\nPolychoric Correlations\nStandardized Regression Coefficients\nUsefulness\nLMG / Shapley Values\nJohnson's Epsilon\nMean Decrease in RF Gini Coefficient\nXGBoost Importance\nNeural Net Permutation Importance\n\n\n\n\n0\ntrust\n0.256\n0.253\n0.136\n0.008\n0.267\n0.156\n0.156\n0.290\n0.155\n\n\n1\nbuild\n0.192\n0.195\n0.023\n0.000\n0.045\n0.102\n0.102\n0.079\n0.096\n\n\n2\ndiffers\n0.185\n0.190\n0.033\n0.001\n0.056\n0.090\n0.090\n0.056\n0.096\n\n\n3\neasy\n0.213\n0.212\n0.026\n0.000\n0.051\n0.100\n0.100\n0.071\n0.097\n\n\n4\nappealing\n0.208\n0.204\n0.040\n0.001\n0.076\n0.086\n0.086\n0.066\n0.103\n\n\n5\nrewarding\n0.195\n0.199\n0.006\n0.000\n0.011\n0.101\n0.101\n0.065\n0.097\n\n\n6\npopular\n0.171\n0.171\n0.019\n0.000\n0.038\n0.095\n0.095\n0.076\n0.102\n\n\n7\nservice\n0.251\n0.252\n0.104\n0.005\n0.199\n0.130\n0.130\n0.111\n0.135\n\n\n8\nimpact\n0.255\n0.261\n0.150\n0.011\n0.255\n0.141\n0.141\n0.185\n0.119"
  },
  {
    "objectID": "blog/project 4/hw4_questions.html#project-summary-and-conclusion",
    "href": "blog/project 4/hw4_questions.html#project-summary-and-conclusion",
    "title": "Machine Learning",
    "section": "Project Summary and Conclusion",
    "text": "Project Summary and Conclusion\nThis project demonstrated the application of key marketing analytics techniques using real-world and synthetic datasets. We implemented k-means clustering from scratch and compared it to scikit-learn’s implementation, visualizing the clustering process and evaluating cluster quality using WCSS and silhouette scores. The analysis highlighted the trade-offs in selecting the optimal number of clusters, with different metrics suggesting different values for K.\nFor the key drivers analysis, we applied a comprehensive set of statistical and machine learning methods—including correlations, regression coefficients, Shapley values, Johnson’s relative weights, and feature importances from random forest, XGBoost, and neural networks—to quantify the importance of predictors for customer satisfaction. The results were summarized in a comparative table, providing a holistic view of variable importance across multiple approaches.\nConclusion:\n- Custom and built-in k-means implementations produced similar clustering results, validating our understanding of the algorithm. - The optimal number of clusters depends on the chosen metric and business context. - Key drivers analysis revealed consistent patterns of predictor importance across methods, but also highlighted the value of using multiple techniques to gain a robust understanding of what drives customer satisfaction."
  }
]